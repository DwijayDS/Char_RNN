{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy implemention char rnn for Harry Potter Books. \n",
    "\n",
    "\n",
    "Inspired by Andrej Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import libraries'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from clean_text import clean_sent\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: dwijayds (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">quiet-dust-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/dwijayds/Char_RNN\" target=\"_blank\">https://wandb.ai/dwijayds/Char_RNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/dwijayds/Char_RNN/runs/2i1kmdki\" target=\"_blank\">https://wandb.ai/dwijayds/Char_RNN/runs/2i1kmdki</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\QWERTY\\Desktop\\Char_RNN\\wandb\\run-20210706_002410-2i1kmdki</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2i1kmdki)</h1><iframe src=\"https://wandb.ai/dwijayds/Char_RNN/runs/2i1kmdki\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x20780db4640>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Incorporating WandB\n",
    "'''\n",
    "wandb.init(project='Char_RNN', entity='dwijayds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNCLEANED Harry potter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 6218823 characters, 106 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "'''Load  books Data'''\n",
    "data = open('harry_txt/harry_data.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d of them are unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Defining Hyperparameters'''\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# WandB\n",
    "config = wandb.config\n",
    "config.learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-0d678b9cf487>, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-0d678b9cf487>\"\u001b[1;36m, line \u001b[1;32m77\u001b[0m\n\u001b[1;33m    def backward\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class min_char_rnn():\n",
    "    \n",
    "    '''\n",
    "    Single Hidden Layer RNN \n",
    "    \n",
    "    \n",
    "    Long sequences might lead to memory overload and perfromance issues because we need to go through the whole\n",
    "    sequence before we back propogate. So we devide the training into different blocks of sequences.\n",
    "    \n",
    "    \n",
    "    The whole code below is meant to be run in batches of sequence.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        \n",
    "        '''Initializing Weight, biases and Hyperparameters'''\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "        self.bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "        self.by = np.zeros((vocab_size, 1)) # output bias\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        \n",
    "        return np.exp(x)/np.sum(np.exp(x))\n",
    "    \n",
    "    def loss(self,inp, target, pre_h):\n",
    "        \n",
    "        '''\n",
    "        Function to return loss, gradient and hidden state\n",
    "        '''\n",
    "        # Defining variables for one pass through rnn\n",
    "        self.xs, self.hs, self.ys, self.ps = {}, {}, {}, {} \n",
    "        # Initializing hidden state for this sequence\n",
    "        self.hs[-1] = pre_h\n",
    "        # Initialize loss\n",
    "        loss = 0\n",
    "        \n",
    "        # forward pass\n",
    "        loss = self.forward(inp, target, pre_h)\n",
    "        \n",
    "        # Backward pass (Calculating gradient)\n",
    "        self.backward(inp,target,pre_h)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, inp, target, pre_h):\n",
    "        '''\n",
    "        Function for forward pass\n",
    "        '''\n",
    "        # Initialize loss\n",
    "        loss = 0\n",
    "        \n",
    "        # Iterating through the sequence\n",
    "        for t in range(len(inp)):\n",
    "            \n",
    "            # one hot encoding the input\n",
    "            self.xs[t] = np.zeros((self.vocab_size,1))\n",
    "            self.xs[t][inp[t]] = 1\n",
    "            \n",
    "            # Update next hidden state (tanh activation)\n",
    "            self.hs[t] = np.tanh(np.dot(self.Wxh, self.xs[t])  +  np.dot(self.Whh, self.hs[t-1])  +   self.bh)\n",
    "            \n",
    "            # Updating Output\n",
    "            self.ys[t] = np.dot(self.Why, self.hs[t]) + self.by\n",
    "            \n",
    "            # Find Output probability - Softmax\n",
    "            self.ps[t] = self.softmax(self.ys[t])\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss += -np.log(self.ps[t][target[t],0])\n",
    "            \n",
    "        # Return Loss\n",
    "        return loss \n",
    "    \n",
    "    def backward(self, imp, target, prev_h):\n",
    "        '''\n",
    "        Function for backward pass\n",
    "        '''\n",
    "        # Initialize gradients\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        \n",
    "        # \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
